---
title: "Greg Durrett — SBU NLP Speaker Series"
speaker: "Greg Durrett"
affiliation: "New York University"
date: "2025-09-18T14:00:00-15:00"     # actual talk time
publishDate: "2025-09-01T00:00:00-04:00"  # ensures page is published now
location: "NCS 120, also accessible on [**zoom**](https://stonybrook.zoom.us/j/95240619421?pwd=Xh3uafNBaQOrC5O3DyjCidLE9jLkS9.1) "
summary: "LLM Reasoning Beyond Scaling"
abstract: >
  Large reasoning models have demonstrated capabilities to solve competition-level math problems, answer “deep research” questions, and address complex coding needs. Much of this progress has been enabled by scaling of data: pre-training data to learn vast knowledge, fine-tuning data to learn natural language reasoning, and RL environments to refine that reasoning. In this talk, I will describe the current LLM reasoning paradigm, its boundaries, and the future of LLM reasoning beyond scaling. First, I will describe the state of reasoning models and where I think scaling can lead to some additional (though perhaps limited) successes. I will then shift to discussing more fundamental issues with models that scale will not resolve in the next few years. I will touch on four current limitations: outdated knowledge, generator-validator gaps, limited creativity, and poor compositional generalization. In all cases, fundamental limitations of LLMs or of supervised learning in general make these problems challenging, inviting future study and novel solutions beyond scaling.
featured: true
draft: false

authors: []
tags: ["Speaker Series"]

event: "SBU NLP Speaker Series"

image:
  caption: "Speaker: Greg Durrett"
  focal_point: Center

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---
[**Greg Durrett**](https://gregdurrett.github.io/) is an associate professor in the Department of Computer Science and the Center for Data Science at New York University. His research is broadly in the areas of natural language processing and machine learning. Currently, his group’s focus is on reasoning about knowledge in text, verifying correctness of generation methods, and studying how to make progress on problems that defy LLM scaling. He is a 2023 Sloan Research Fellow and a recipient of a 2022 NSF CAREER award. He has served in numerous roles for ACL conferences, recently as a member of the NAACL Board since 2024 and as Senior Area Chair for ACL 2025 and EMNLP 2025. He received his BS in Computer Science and Mathematics from MIT and his PhD in Computer Science from UC Berkeley, where he was advised by Dan Klein.